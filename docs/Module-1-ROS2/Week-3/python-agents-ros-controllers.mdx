---
title: "Python Agents → ROS Controllers"
sidebar_position: 3
---

# Python Agents → ROS Controllers

## Learning Objectives

By the end of this chapter, you should be able to:
- Design Python-based agent architectures for robotics applications
- Integrate Python agents with ROS 2 control systems
- Implement action-based control interfaces for agents
- Create decision-making systems that interact with ROS controllers
- Handle real-time constraints in agent-controller communication
- Implement monitoring and safety mechanisms for agent-controlled systems
- Design agent learning interfaces for control improvement

## Introduction to Agent-Based ROS Control

The integration of intelligent agents with ROS (Robot Operating System) controllers represents a powerful approach to creating autonomous robotic systems. Python-based agents can leverage advanced AI and machine learning libraries while interfacing with ROS's robust control and communication infrastructure. This chapter explores the patterns and techniques for effectively connecting AI agents with ROS-based control systems.

## Agent Architecture for Robotics

### Agent-Environment Interface

In robotics, agents must interact with physical environments through ROS:
- **Perception Interface**: Convert sensor data from ROS topics to agent observations
- **Action Interface**: Translate agent decisions into ROS commands
- **Reward Interface**: Map environmental feedback to agent learning signals
- **State Representation**: Maintain internal state consistent with ROS world models

### Types of Robotics Agents

Different agent architectures serve different purposes:

**Reactive Agents**:
- Direct mapping from perceptions to actions
- Simple, reliable, but limited in complex tasks
- Good for basic sensorimotor behaviors

**Deliberative Agents**:
- Planning-based approaches with world models
- Complex reasoning but computationally intensive
- Suitable for navigation and task planning

**Learning Agents**:
- Adapt through experience and feedback
- Use ML techniques like reinforcement learning
- Improve performance over time with experience

**Hybrid Agents**:
- Combine multiple architectural approaches
- Leverage strengths of different methods
- Most effective for complex robotic tasks

## Python Agent Implementation Patterns

### Basic Agent Structure

A standard Python agent template for ROS integration:

```python
import rclpy
from rclpy.node import Node
import numpy as np
from typing import Dict, Any, Optional

class RoboticsAgent(Node):
    def __init__(self, agent_name: str):
        super().__init__(agent_name)
        
        # Agent state
        self.state_vector = None
        self.action_history = []
        self.belief_state = {}
        
        # ROS interfaces
        self.sensor_subscribers = {}
        self.controller_publishers = {}
        
        # Agent parameters
        self.agent_config = {
            'learning_rate': 0.001,
            'discount_factor': 0.99,
            'exploration_rate': 0.1
        }
        
    def process_observation(self, sensor_data: Dict[str, Any]) -> np.ndarray:
        """Convert ROS sensor data to agent-usable state representation"""
        # Convert raw sensor readings to processed state vector
        # Normalize/standardize inputs
        # Update internal state representation
        pass
    
    def select_action(self, state: np.ndarray) -> Dict[str, Any]:
        """Select action based on current state"""
        # Implement action selection logic
        # Could be rule-based, learning-based, or planning-based
        pass
    
    def execute_action(self, action: Dict[str, Any]) -> bool:
        """Execute action through ROS interface"""
        # Convert action to ROS commands
        # Send commands to appropriate controllers
        # Monitor execution status
        pass
    
    def update_agent(self, reward: float, next_state: np.ndarray):
        """Update agent based on experience"""
        # Update learning models if applicable
        # Adjust internal parameters
        # Log learning progress
        pass
```

### Sensor Data Processing

Converting ROS sensor data to agent observations:

```python
import sensor_msgs.msg as sensor_msgs
import geometry_msgs.msg as geometry_msgs
import nav_msgs.msg as nav_msgs
from std_msgs.msg import Float64MultiArray
import tf2_ros
import cv2
from cv_bridge import CvBridge

class SensorProcessor:
    def __init__(self, node: Node):
        self.node = node
        self.cv_bridge = CvBridge()
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, node)
        
        # Initialize sensor subscribers
        self.camera_sub = node.create_subscription(
            sensor_msgs.Image, '/camera/rgb/image_raw', 
            self.camera_callback, 10)
        self.laser_sub = node.create_subscription(
            sensor_msgs.LaserScan, '/scan', 
            self.laser_callback, 10)
        self.odom_sub = node.create_subscription(
            nav_msgs.Odometry, '/odom', 
            self.odom_callback, 10)
        
        # Processed sensor data
        self.latest_image = None
        self.laser_ranges = None
        self.robot_pose = None
        
    def camera_callback(self, msg: sensor_msgs.Image):
        """Process camera image for agent perception"""
        try:
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")
            # Process image for agent (resize, crop, feature extraction)
            self.latest_image = self.preprocess_image(cv_image)
        except Exception as e:
            self.node.get_logger().error(f"Camera processing error: {e}")
    
    def laser_callback(self, msg: sensor_msgs.LaserScan):
        """Process laser scan data for agent perception"""
        # Convert scan to agent-friendly format
        ranges = np.array(msg.ranges)
        ranges = np.clip(ranges, msg.range_min, msg.range_max)
        self.laser_ranges = ranges
    
    def odom_callback(self, msg: nav_msgs.Odometry):
        """Process odometry data for agent state"""
        pose = msg.pose.pose
        self.robot_pose = {
            'x': pose.position.x,
            'y': pose.position.y,
            'theta': self.quaternion_to_yaw(pose.orientation)
        }
    
    def preprocess_image(self, image):
        """Prepare image for agent processing"""
        # Resize image to standard dimensions
        resized = cv2.resize(image, (224, 224))
        # Normalize pixel values
        normalized = resized.astype(np.float32) / 255.0
        return normalized
    
    def get_agent_state(self) -> np.ndarray:
        """Combine all sensor data into agent state"""
        # Concatenate sensor readings into state vector
        state_parts = []
        
        if self.laser_ranges is not None:
            # Use only relevant laser ranges for state
            relevant_ranges = self.laser_ranges[::10]  # Every 10th reading
            state_parts.append(relevant_ranges)
        
        if self.robot_pose is not None:
            pose_data = [self.robot_pose['x'], self.robot_pose['y'], self.robot_pose['theta']]
            state_parts.append(np.array(pose_data))
        
        if self.latest_image is not None:
            # Flatten and append processed image features
            img_features = self.latest_image.flatten()
            state_parts.append(img_features[:1000])  # Limit image features to manage state size
        
        return np.concatenate(state_parts)
```

## Controller Integration Patterns

### Joint Trajectory Controllers

Integrating with ROS 2 control systems:

```python
import rclpy
from rclpy.action import ActionClient
from control_msgs.action import FollowJointTrajectory
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
import time

class RobotController:
    def __init__(self, node: Node):
        self.node = node
        self.joint_names = ['joint1', 'joint2', 'joint3']  # Example joint names
        
        # Action client for trajectory execution
        self.trajectory_client = ActionClient(
            node, 
            FollowJointTrajectory, 
            '/joint_trajectory_controller/follow_joint_trajectory'
        )
        
        # Publisher for direct velocity commands (if needed)
        self.velocity_pub = node.create_publisher(
            Float64MultiArray, 
            '/velocity_controller/commands', 
            10
        )
    
    def execute_trajectory(self, joint_positions: list, duration: float = 5.0):
        """Execute a joint trajectory using ROS 2 controllers"""
        goal_msg = FollowJointTrajectory.Goal()
        
        # Create trajectory message
        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names
        
        point = JointTrajectoryPoint()
        point.positions = joint_positions
        point.time_from_start.sec = int(duration)
        point.time_from_start.nanosec = int((duration - int(duration)) * 1e9)
        
        trajectory.points.append(point)
        goal_msg.trajectory = trajectory
        
        # Wait for action server
        if not self.trajectory_client.wait_for_server(timeout_sec=5.0):
            self.node.get_logger().error("Trajectory server not available")
            return False
        
        # Send goal
        future = self.trajectory_client.send_goal_async(goal_msg)
        return future
    
    def send_velocity_command(self, velocities: list):
        """Send direct velocity commands to joints"""
        cmd_msg = Float64MultiArray()
        cmd_msg.data = velocities
        self.velocity_pub.publish(cmd_msg)
    
    def convert_agent_action_to_control(self, action: Dict[str, Any]):
        """Transform agent action to ROS control command"""
        # Example: Convert high-level action to specific control command
        if action['type'] == 'move_arm':
            joint_positions = action['joint_positions']
            return self.execute_trajectory(joint_positions)
        elif action['type'] == 'move_base':
            velocity_cmd = action['velocity']
            return self.send_velocity_command(velocity_cmd)
        else:
            self.node.get_logger().warn(f"Unknown action type: {action['type']}")
```

### Action-Based Control Interface

Creating a standardized interface between agents and controllers:

```python
from enum import Enum
from dataclasses import dataclass
from typing import Union, List

class ActionType(Enum):
    MOVE_TO_POSE = "move_to_pose"
    MOVE_JOINTS = "move_joints"
    EXECUTE_TRAJECTORY = "execute_trajectory"
    GRIPPER_CONTROL = "gripper_control"
    NAVIGATE = "navigate"
    STOP = "stop"

@dataclass
class AgentAction:
    action_type: ActionType
    parameters: Dict[str, Any]
    priority: int = 0  # Higher number = higher priority
    timeout: float = 10.0  # Execution timeout in seconds

class AgentControllerInterface:
    def __init__(self, node: Node):
        self.node = node
        self.active_goals = {}
        self.controller_status = {}
        
        # Create action clients for different controllers
        self.arm_client = ActionClient(
            node, FollowJointTrajectory, 
            '/arm_controller/follow_joint_trajectory'
        )
        self.base_client = ActionClient(
            node, NavigateToPose, 
            '/navigate_to_pose'
        )
        
    def execute_agent_action(self, agent_action: AgentAction) -> bool:
        """Execute an agent action through appropriate controller"""
        try:
            if agent_action.action_type == ActionType.MOVE_TO_POSE:
                return self._execute_move_to_pose(agent_action.parameters)
            elif agent_action.action_type == ActionType.MOVE_JOINTS:
                return self._execute_move_joints(agent_action.parameters)
            elif agent_action.action_type == ActionType.GRIPPER_CONTROL:
                return self._execute_gripper_control(agent_action.parameters)
            elif agent_action.action_type == ActionType.NAVIGATE:
                return self._execute_navigate(agent_action.parameters)
            elif agent_action.action_type == ActionType.STOP:
                return self._execute_stop()
            else:
                self.node.get_logger().error(f"Unknown action type: {agent_action.action_type}")
                return False
        except Exception as e:
            self.node.get_logger().error(f"Action execution failed: {e}")
            return False
    
    def _execute_move_to_pose(self, params: Dict[str, Any]) -> bool:
        """Execute move to pose action"""
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose = params['pose']  # geometry_msgs.PoseStamped
        return self._send_action_goal(self.base_client, goal_msg, params.get('timeout', 30.0))
    
    def _execute_move_joints(self, params: Dict[str, Any]) -> bool:
        """Execute joint movement action"""
        trajectory = JointTrajectory()
        trajectory.joint_names = params['joint_names']
        
        point = JointTrajectoryPoint()
        point.positions = params['positions']
        point.time_from_start.sec = int(params.get('duration', 5.0))
        
        goal_msg = FollowJointTrajectory.Goal()
        goal_msg.trajectory = trajectory
        return self._send_action_goal(self.arm_client, goal_msg, params.get('timeout', 10.0))
    
    def _send_action_goal(self, client, goal_msg, timeout: float):
        """Send action goal and wait for result"""
        if not client.wait_for_server(timeout_sec=timeout/2):
            self.node.get_logger().error("Action server not available")
            return False
        
        future = client.send_goal_async(goal_msg)
        # Add timeout handling here
        return True
```

## Real-Time Considerations

### Timing and Synchronization

Managing real-time constraints in agent-controller communication:

```python
import threading
import time
from collections import deque
import queue

class RealTimeAgentInterface:
    def __init__(self, node: Node, control_frequency: float = 10.0):
        self.node = node
        self.control_frequency = control_frequency
        self.dt = 1.0 / control_frequency
        
        # Data queues for sensor data
        self.sensor_queue = queue.Queue(maxsize=10)
        self.action_queue = queue.Queue(maxsize=1)
        
        # Timing control
        self.last_update_time = time.time()
        self.update_period = 1.0 / control_frequency
        
        # Real-time thread for control loop
        self.control_thread = None
        self.running = False
        
    def start_control_loop(self):
        """Start the real-time control loop"""
        self.running = True
        self.control_thread = threading.Thread(target=self._control_loop)
        self.control_thread.start()
    
    def _control_loop(self):
        """Real-time control loop"""
        while self.running:
            start_time = time.time()
            
            try:
                # Get latest sensor data
                while not self.sensor_queue.empty():
                    sensor_data = self.sensor_queue.get()
                
                # Process with agent
                if hasattr(self, 'agent'):
                    action = self.agent.get_action(sensor_data)
                    
                    # Enqueue action for execution
                    try:
                        self.action_queue.put_nowait(action)
                    except queue.Full:
                        # Drop old action if queue is full
                        pass
                
                # Execute action through controller
                if not self.action_queue.empty():
                    action = self.action_queue.get()
                    self.execute_controller_command(action)
                
            except Exception as e:
                self.node.get_logger().error(f"Control loop error: {e}")
            
            # Maintain timing
            elapsed = time.time() - start_time
            sleep_time = self.update_period - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)
    
    def stop_control_loop(self):
        """Stop the real-time control loop"""
        self.running = False
        if self.control_thread:
            self.control_thread.join()
```

## Learning Integration

### Reinforcement Learning with ROS

Connecting RL agents to ROS control systems:

```python
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random

class RLAgent:
    def __init__(self, state_dim: int, action_dim: int, lr: float = 0.001):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = lr
        
        # Neural network for policy
        self.policy_net = self._build_network(state_dim, action_dim)
        self.target_net = self._build_network(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)
        
        # Experience replay buffer
        self.memory = deque(maxlen=10000)
        self.batch_size = 32
        
        # Exploration parameters
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
        # Training parameters
        self.gamma = 0.95  # Discount factor
        
    def _build_network(self, state_dim: int, action_dim: int) -> nn.Module:
        """Build neural network for policy"""
        class PolicyNetwork(nn.Module):
            def __init__(self, input_dim: int, output_dim: int):
                super().__init__()
                self.network = nn.Sequential(
                    nn.Linear(input_dim, 256),
                    nn.ReLU(),
                    nn.Linear(256, 256),
                    nn.ReLU(),
                    nn.Linear(256, output_dim)
                )
            
            def forward(self, x):
                return self.network(x)
        
        return PolicyNetwork(state_dim, action_dim)
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """Select action using epsilon-greedy policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        if training and random.random() < self.epsilon:
            # Random action for exploration
            return random.randrange(self.action_dim)
        
        # Greedy action from policy
        with torch.no_grad():
            q_values = self.policy_net(state_tensor)
            return q_values.argmax().item()
    
    def store_transition(self, state: np.ndarray, action: int, 
                        reward: float, next_state: np.ndarray, done: bool):
        """Store experience in replay buffer"""
        self.memory.append((state, action, reward, next_state, done))
    
    def train(self):
        """Train the agent using experience replay"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample random batch from memory
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.BoolTensor(dones)
        
        # Compute current Q values
        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))
        
        # Compute next Q values using target network
        next_q_values = self.target_net(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # Compute loss
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        # Backpropagate
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # Decay exploration
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

class RLRosInterface:
    def __init__(self, node: Node, state_dim: int, action_dim: int):
        self.node = node
        self.agent = RLAgent(state_dim, action_dim)
        
        # ROS interfaces
        self.sensor_processor = SensorProcessor(node)
        self.controller = RobotController(node)
        
        # Episode tracking
        self.episode_reward = 0
        self.episode_step = 0
        self.max_episode_steps = 1000
        
        # Reward calculation
        self.reward_calculator = RewardCalculator()
        
    def agent_step(self):
        """Execute one step of agent interaction"""
        # Get current state from sensors
        current_state = self.sensor_processor.get_agent_state()
        
        # Select action using trained policy
        action_idx = self.agent.select_action(current_state, training=True)
        action = self._action_index_to_ros_command(action_idx)
        
        # Execute action
        success = self.controller.convert_agent_action_to_control(action)
        
        if success:
            # Wait for execution to complete or timeout
            time.sleep(0.1)  # Simple wait, in practice use action feedback
            
            # Get reward and next state
            reward = self.reward_calculator.calculate_reward()
            next_state = self.sensor_processor.get_agent_state()
            
            # Calculate if episode is done
            done = self.episode_step >= self.max_episode_steps or self.check_termination_conditions()
            
            # Store experience for training
            self.agent.store_transition(
                current_state, action_idx, reward, next_state, done
            )
            
            # Train agent
            self.agent.train()
            
            # Update episode tracking
            self.episode_reward += reward
            self.episode_step += 1
            
            if done:
                self.reset_episode()
    
    def _action_index_to_ros_command(self, action_idx: int) -> Dict[str, Any]:
        """Convert action index to ROS command"""
        # Example: Discrete actions for robot control
        action_map = {
            0: {'type': 'move_base', 'velocity': [0.1, 0.0, 0.0]},  # Forward
            1: {'type': 'move_base', 'velocity': [-0.1, 0.0, 0.0]},  # Backward
            2: {'type': 'move_base', 'velocity': [0.0, 0.0, 0.1]},  # Turn left
            3: {'type': 'move_base', 'velocity': [0.0, 0.0, -0.1]},  # Turn right
            4: {'type': 'stop'}  # Stop
        }
        return action_map[action_idx]
```

## Safety and Monitoring

### Safety Layer Implementation

Creating a safety layer between agents and controllers:

```python
class SafetyLayer:
    def __init__(self, node: Node):
        self.node = node
        self.safety_enabled = True
        
        # Safety constraints
        self.joint_limits = {}  # Map of joint limits
        self.velocity_limits = {}  # Velocity constraints
        self.workspace_limits = {}  # Cartesian workspace constraints
        self.force_limits = {}  # Force/torque constraints
        
        # Emergency stop publisher
        self.emergency_stop_pub = node.create_publisher(
            Bool, '/emergency_stop', 10
        )
    
    def check_action_safety(self, action: Dict[str, Any]) -> bool:
        """Check if action is safe to execute"""
        if not self.safety_enabled:
            return True
        
        try:
            # Check joint position limits
            if 'joint_positions' in action:
                for i, pos in enumerate(action['joint_positions']):
                    joint_name = f'joint_{i}'
                    if joint_name in self.joint_limits:
                        limits = self.joint_limits[joint_name]
                        if pos < limits['min'] or pos > limits['max']:
                            self.node.get_logger().warn(f"Joint position limit violation for {joint_name}")
                            return False
            
            # Check velocity limits
            if 'velocities' in action:
                for i, vel in enumerate(action['velocities']):
                    joint_name = f'joint_{i}'
                    if joint_name in self.velocity_limits:
                        max_vel = self.velocity_limits[joint_name]
                        if abs(vel) > max_vel:
                            self.node.get_logger().warn(f"Velocity limit violation for {joint_name}")
                            return False
            
            # Add more safety checks as needed
            return True
            
        except Exception as e:
            self.node.get_logger().error(f"Safety check error: {e}")
            return False
    
    def emergency_stop(self):
        """Emergency stop for robot"""
        stop_msg = Bool()
        stop_msg.data = True
        self.emergency_stop_pub.publish(stop_msg)

class SafeAgentController:
    def __init__(self, node: Node):
        self.node = node
        self.safety_layer = SafetyLayer(node)
        self.agent_controller = AgentControllerInterface(node)
        
    def execute_safe_action(self, agent_action: AgentAction) -> bool:
        """Execute agent action with safety checks"""
        # Check if action is safe
        if not self.safety_layer.check_action_safety(agent_action.parameters):
            self.node.get_logger().error("Action failed safety check")
            return False
        
        # Execute action
        return self.agent_controller.execute_agent_action(agent_action)
```

## Performance Optimization

### Efficient Agent-Controller Communication

Optimizing the communication between Python agents and ROS controllers:

```python
import asyncio
import concurrent.futures
from threading import Lock

class OptimizedAgentController:
    def __init__(self, node: Node):
        self.node = node
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
        self.lock = Lock()
        
        # Asynchronous interfaces
        self.sensor_future = None
        self.action_future = None
        
        # Caching for frequently accessed data
        self.cached_states = {}
        self.state_cache_size = 100
        
    async def async_sensor_processing(self):
        """Asynchronously process sensor data"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor, 
            self._process_sensor_data
        )
    
    def _process_sensor_data(self):
        """Process sensor data in separate thread"""
        # Heavy sensor processing here
        pass
    
    def execute_action_batch(self, actions: List[AgentAction]):
        """Execute multiple actions efficiently"""
        futures = []
        for action in actions:
            future = self.executor.submit(
                self._execute_single_action, 
                action
            )
            futures.append(future)
        
        # Wait for all actions to complete
        results = [future.result() for future in futures]
        return results
    
    def _execute_single_action(self, action: AgentAction):
        """Execute a single action"""
        with self.lock:
            # Thread-safe execution
            return self._execute_action_internal(action)
```

## Exercises and Self-Check

1. **Implementation Exercise**: Create a Python agent that navigates a robot to a goal position using laser scan data. Integrate with ROS 2 navigation stack.

2. **Controller Design**: Design a controller interface that can handle both high-level task planning actions and low-level motion control commands.

3. **Safety Integration**: Implement a safety layer that prevents collisions based on laser scan data and enforces joint limits.

4. **Learning System**: Create a simple Q-learning agent that learns to avoid obstacles using laser scan data and movement commands.

5. **Real-time Constraint**: Design a real-time interface that maintains 50Hz control rate while processing sensor data and executing agent decisions.

## Summary

The integration of Python-based agents with ROS controllers enables sophisticated autonomous robotic systems that can leverage advanced AI and machine learning techniques. This integration requires careful attention to timing, safety, and communication patterns to ensure reliable and safe operation.

Key considerations include proper state representation for agents, safe action execution through ROS controllers, real-time performance requirements, and safety mechanisms to prevent dangerous robot behavior. The patterns and techniques described in this chapter provide a foundation for creating robust agent-controller systems suitable for real-world robotic applications.

By combining the flexibility and power of Python-based AI agents with ROS's proven control and communication infrastructure, developers can create intelligent robotic systems capable of complex autonomous behaviors in dynamic environments.

---

**Keywords**: Python Agents, ROS Controllers, Agent Integration, Control Systems, Robotics AI, Real-time Systems